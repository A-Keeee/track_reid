# æ–‡ä»¶å: track_torch_ros.py
# æè¿°: é€šè¿‡è®¢é˜…ROS2è¯é¢˜è·å–å›¾åƒï¼Œè‡ªåŠ¨é€‰æ‹©ä¸­å¿ƒç›®æ ‡è¿›è¡Œè·Ÿè¸ªã€‚
# ç‰ˆæœ¬: v5.0 - ROS2é›†æˆç‰ˆ
#
# è¿è¡Œä¾èµ–:
# - rclpy
# - cv_bridge
# - sensor_msgs
# - message_filters

# ros2 topic pub /start_vision std_msgs/msg/Int32 "{data: 1}"
import cv2
import numpy as np
from ultralytics import YOLO
import time
import math
import sys
import os
import threading
import queue
import grpc
import argparse
import torch
import torch.nn.functional as F
from pathlib import Path
from PIL import Image
import json

# ==============================================================================
# ROS2 ç›¸å…³å¯¼å…¥
# ==============================================================================
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image as RosImage
from cv_bridge import CvBridge, CvBridgeError
import message_filters

# å¯¼å…¥ç”Ÿæˆçš„gRPCæ¨¡å—
try:
    import tracking_pb2
    import tracking_pb2_grpc
except ImportError:
    print("è­¦å‘Š: æœªæ‰¾åˆ°gRPCæ¨¡å—ï¼ŒgRPCé€šä¿¡åŠŸèƒ½å°†è¢«ç¦ç”¨")
    print("è¯·è¿è¡Œ: python -m grpc_tools.protoc --python_out=. --grpc_python_out=. --proto_path=. tracking.proto")
    tracking_pb2 = None
    tracking_pb2_grpc = None

# ReID ç›¸å…³å¯¼å…¥
from reid.data.transforms import build_transforms
from reid.config import cfg as reidCfg
from reid.modeling import build_model
from utils.plotting import plot_one_box

# å¯¼å…¥æ‰©å±•å¡å°”æ›¼æ»¤æ³¢å™¨
from extended_kalman_filter import ExtendedKalmanFilter3D, AdaptiveEKF3D, EnhancedEKF3D


# ==============================================================================
# å§¿æ€å¯è§†åŒ–ç›¸å…³
# ==============================================================================

# COCO æ ¼å¼çš„äººä½“éª¨æ¶è¿æ¥ (17ä¸ªå…³é”®ç‚¹)
SKELETON_CONNECTIONS = [
    # å¤´éƒ¨
    (0, 1), (0, 2), (1, 3), (2, 4),  # é¼»å­-çœ¼ç›-è€³æœµ
    # èº«ä½“ä¸­è½´çº¿
    (5, 6), (5, 11), (6, 12), (11, 12),  # è‚©è†€-é«‹éƒ¨
    # å·¦è‡‚
    (5, 7), (7, 9),  # å·¦è‚©-å·¦è‚˜-å·¦è…•
    # å³è‡‚
    (6, 8), (8, 10),  # å³è‚©-å³è‚˜-å³è…•
    # å·¦è…¿
    (11, 13), (13, 15),  # å·¦é«‹-å·¦è†-å·¦è¸
    # å³è…¿
    (12, 14), (14, 16)   # å³é«‹-å³è†-å³è¸
]

# å…³é”®ç‚¹é¢œè‰² (BGRæ ¼å¼)
KEYPOINT_COLORS = [
    (255, 0, 0),    # 0: é¼»å­ - çº¢è‰²
    (255, 85, 0),   # 1: å·¦çœ¼ - æ©™çº¢è‰²
    (255, 170, 0),  # 2: å³çœ¼ - æ©™è‰²
    (255, 255, 0),  # 3: å·¦è€³ - é»„è‰²
    (170, 255, 0),  # 4: å³è€³ - é»„ç»¿è‰²
    (85, 255, 0),   # 5: å·¦è‚© - ç»¿è‰²
    (0, 255, 0),    # 6: å³è‚© - çº¯ç»¿è‰²
    (0, 255, 85),   # 7: å·¦è‚˜ - é’ç»¿è‰²
    (0, 255, 170),  # 8: å³è‚˜ - é’è‰²
    (0, 255, 255),  # 9: å·¦è…• - é’è“è‰²
    (0, 170, 255),  # 10: å³è…• - è“è‰²
    (0, 85, 255),   # 11: å·¦é«‹ - è“ç´«è‰²
    (0, 0, 255),    # 12: å³é«‹ - ç´«è‰²
    (85, 0, 255),   # 13: å·¦è† - ç´«çº¢è‰²
    (170, 0, 255),  # 14: å³è† - ç²‰è‰²
    (255, 0, 255),  # 15: å·¦è¸ - å“çº¢è‰²
    (255, 0, 170)   # 16: å³è¸ - ç«çº¢è‰²
]

def draw_keypoints(image, keypoints, keypoints_conf, conf_threshold=0.1):
    """ç»˜åˆ¶å…³é”®ç‚¹"""
    for i, (kpt, conf) in enumerate(zip(keypoints, keypoints_conf)):
        if conf > conf_threshold:
            x, y = int(kpt[0]), int(kpt[1])
            color = KEYPOINT_COLORS[i] if i < len(KEYPOINT_COLORS) else (255, 255, 255)
            cv2.circle(image, (x, y), 4, color, -1)
            cv2.circle(image, (x, y), 6, (0, 0, 0), 2)  # é»‘è‰²è¾¹æ¡†

def draw_skeleton(image, keypoints, keypoints_conf, conf_threshold=0.1):
    """ç»˜åˆ¶éª¨æ¶è¿æ¥"""
    for connection in SKELETON_CONNECTIONS:
        pt1_idx, pt2_idx = connection
        if (pt1_idx < len(keypoints_conf) and pt2_idx < len(keypoints_conf) and
            keypoints_conf[pt1_idx] > conf_threshold and keypoints_conf[pt2_idx] > conf_threshold):
            
            pt1 = (int(keypoints[pt1_idx][0]), int(keypoints[pt1_idx][1]))
            pt2 = (int(keypoints[pt2_idx][0]), int(keypoints[pt2_idx][1]))
            
            # æ ¹æ®è¿æ¥éƒ¨ä½é€‰æ‹©é¢œè‰²
            if connection in [(5, 6), (5, 11), (6, 12), (11, 12)]:  # èº¯å¹²
                color = (0, 255, 0)  # ç»¿è‰²
            elif connection in [(5, 7), (7, 9)]:  # å·¦è‡‚
                color = (255, 0, 0)  # è“è‰²
            elif connection in [(6, 8), (8, 10)]:  # å³è‡‚
                color = (0, 0, 255)  # çº¢è‰²
            elif connection in [(11, 13), (13, 15)]:  # å·¦è…¿
                color = (255, 255, 0)  # é’è‰²
            elif connection in [(12, 14), (14, 16)]:  # å³è…¿
                color = (0, 255, 255)  # é»„è‰²
            else:  # å¤´éƒ¨
                color = (255, 0, 255)  # å“çº¢è‰²
            
            cv2.line(image, pt1, pt2, color, 2)

def draw_pose_on_person(image, detection, label_prefix="", label_color=(0, 255, 0)):
    """åœ¨æ£€æµ‹åˆ°çš„äººç‰©ä¸Šç»˜åˆ¶å§¿æ€éª¨æ¶å’Œè¾¹ç•Œæ¡†"""
    # ç»˜åˆ¶è¾¹ç•Œæ¡†
    plot_one_box(detection['box'], image, label=label_prefix, color=label_color)
    
    # ç»˜åˆ¶éª¨æ¶å’Œå…³é”®ç‚¹
    draw_skeleton(image, detection['keypoints'], detection['keypoints_conf'])
    draw_keypoints(image, detection['keypoints'], detection['keypoints_conf'])
    
    # ç»˜åˆ¶äººä½“ä¸­å¿ƒç‚¹
    body_center = calculate_body_center_from_keypoints(
        detection['keypoints'], detection['keypoints_conf'], detection['box']
    )
    center_x, center_y = int(body_center[0]), int(body_center[1])
    cv2.circle(image, (center_x, center_y), 8, (0, 255, 255), -1)  # é»„è‰²åœ†ç‚¹
    cv2.circle(image, (center_x, center_y), 10, (0, 0, 0), 2)     # é»‘è‰²è¾¹æ¡†

# ==============================================================================
# åæ ‡å¯¼å‡ºå™¨ (ç”¨äºROS2é›†æˆ)
# ==============================================================================
class CoordinateExporter:
    def __init__(self, export_file='/tmp/tracking_coords.json'):
        self.export_file = export_file
        self.last_coords = None
        
    def export_coordinates(self, coords_tuple):
        """å¯¼å‡ºåæ ‡åˆ°æ–‡ä»¶ï¼Œä¾›ROS2èŠ‚ç‚¹è¯»å–"""
        try:
            if coords_tuple:
                x, y, z = coords_tuple
            else:
                x, y, z = 0.0, 0.0, 0.0
            
            # åªæœ‰åæ ‡å‘ç”Ÿå˜åŒ–æ—¶æ‰å†™å…¥æ–‡ä»¶
            current_coords = (x, y, z)
            if current_coords == self.last_coords:
                return
                
            data = {
                'x': float(x),
                'y': float(y),
                'z': float(z),
                'timestamp': time.time()
            }
            
            with open(self.export_file, 'w') as f:
                json.dump(data, f)
            
            self.last_coords = current_coords
            
        except Exception as e:
            print(f"âŒ å¯¼å‡ºåæ ‡æ—¶å‡ºé”™: {e}")


# ==============================================================================
# gRPC å®¢æˆ·ç«¯
# ==============================================================================
class TrackingGRPCClient:
    def __init__(self, server_address='localhost:50051'):
        self.server_address = server_address
        self.channel = None
        self.stub = None
        self.connected = False
        self.coordinate_queue = queue.Queue(maxsize=100)
        self.stream_thread = None
        self.streaming = False

    def connect(self):
        if not all([tracking_pb2, tracking_pb2_grpc]):
            print("gRPCæ¨¡å—æœªå¯¼å…¥ï¼Œè·³è¿‡è¿æ¥")
            return False
        try:
            self.channel = grpc.insecure_channel(self.server_address)
            grpc.channel_ready_future(self.channel).result(timeout=5)
            self.stub = tracking_pb2_grpc.TrackingServiceStub(self.channel)
            self.connected = True
            print(f"âœ… gRPCå®¢æˆ·ç«¯è¿æ¥æˆåŠŸ: {self.server_address}")
            self.start_coordinate_stream()
            return True
        except Exception as e:
            print(f"âŒ gRPCè¿æ¥å¼‚å¸¸: {e}")
            self.connected = False
            return False

    def start_coordinate_stream(self):
        if not self.connected: return

        def coordinate_generator():
            while self.streaming:
                try:
                    coordinate = self.coordinate_queue.get(timeout=5.0)
                    yield coordinate
                except queue.Empty:
                    continue

        def stream_worker():
            try:
                self.streaming = True
                print("ğŸ“¡ åæ ‡æµä¼ è¾“å·²å¯åŠ¨...")
                self.stub.SendCoordinates(coordinate_generator())
            except grpc.RpcError as e:
                if e.code() != grpc.StatusCode.CANCELLED:
                    print(f"âŒ åæ ‡æµä¼ è¾“RPCå¤±è´¥: {e.code()} - {e.details()}")
            finally:
                self.streaming = False
                print("ğŸ“¡ åæ ‡æµä¼ è¾“å·²åœæ­¢ã€‚")

        self.stream_thread = threading.Thread(target=stream_worker, daemon=True)
        self.stream_thread.start()

    def disconnect(self):
        self.streaming = False
        if self.channel:
            self.channel.close()
        if self.stream_thread and self.stream_thread.is_alive():
            self.stream_thread.join(timeout=2)
        self.connected = False
        print("gRPCå®¢æˆ·ç«¯å·²æ–­å¼€è¿æ¥")

    def send_target_coordinates(self, coords_tuple):
        if not self.connected or not self.streaming: return

        try:
            if coords_tuple:
                x, y, z = coords_tuple
            else:
                x, y, z = 0.0, 0.0, 0.0
            
            coordinate_msg = tracking_pb2.CoordinateData(x=float(x), y=float(y), z=float(z))
            
            if self.coordinate_queue.full():
                self.coordinate_queue.get_nowait()
            self.coordinate_queue.put_nowait(coordinate_msg)
        except Exception as e:
            print(f"âŒ å‘é€åæ ‡åˆ°é˜Ÿåˆ—æ—¶å‡ºé”™: {e}")

    def get_command_state(self):
        if not self.connected:
            return False, 0
        try:
            status = self.stub.GetTrackingStatus(tracking_pb2.Empty(), timeout=0.5)
            return status.is_active, status.target_id
        except grpc.RpcError:
            return False, 0


# ==============================================================================
# æ ¸å¿ƒé€»è¾‘
# ==============================================================================
def calculate_3d_coordinates(depth_map, center_point, size=None):
    u, v = int(center_point[0]), int(center_point[1])
    height, width = depth_map.shape
    w, h = (10, 10) if size is None else size
    roi_size = max(5, int(min(w, h) * 0.1))
    x1, y1 = max(0, u - roi_size), max(0, v - roi_size)
    x2, y2 = min(width - 1, u + roi_size), min(height - 1, v + roi_size)
    if x1 >= x2 or y1 >= y2: return (0, 0, 0)
    
    depth_roi = depth_map[int(y1):int(y2), int(x1):int(x2)]
    
    # æ·±åº¦å€¼å•ä½å·²ç»æ˜¯ç±³ (32FC1)ï¼Œæ‰€ä»¥ä¸éœ€è¦é™¤ä»¥1000
    # è¿‡æ»¤æ‰æ— æ•ˆçš„æ·±åº¦å€¼ (0 æˆ– NaN)
    valid_mask = (depth_roi > 0.3) & (depth_roi < 15.0) & ~np.isnan(depth_roi)
    
    if not np.any(valid_mask): return (0, 0, 0)
    
    median_depth = np.median(depth_roi[valid_mask])
    Z_cam = median_depth
    
    if Z_cam <= 0.3 or Z_cam > 15.0: return (0, 0, 0)
    
    # ä½¿ç”¨Odinç›¸æœºçš„å†…å‚
    fx, fy = 734.357, 734.629
    cx, cy = 816.469, 642.979
    
    try:
        X_cam = (u - cx) * Z_cam / fx
        Y_cam = (v - cy) * Z_cam / fy
        # åæ ‡ç³»è½¬æ¢ï¼š(ç›¸æœºåæ ‡ç³» -> ä¸–ç•Œ/æœºå™¨äººåæ ‡ç³»)
        # X_world -> å‰æ–¹, Y_world -> å·¦æ–¹, Z_world -> ä¸Šæ–¹
        X_world = Z_cam
        Y_world = -X_cam
        Z_world = -Y_cam
    except ZeroDivisionError: return (0, 0, 0)
    
    if any(math.isnan(val) for val in (X_world, Y_world, Z_world)): return (0, 0, 0)
    
    return (X_world, Y_world, Z_world)

def detect_all_poses(frame, model, conf_thres=0.5):
    """æ£€æµ‹æ‰€æœ‰äººçš„å§¿æ€ï¼Œè¿”å›åŒ…å«è¾¹ç•Œæ¡†å’Œå…³é”®ç‚¹çš„æ£€æµ‹ç»“æœ"""
    results = model.predict(source=frame, show=False, classes=[0], conf=conf_thres, verbose=False)
    detections = []
    if len(results[0].boxes) > 0 and results[0].keypoints is not None:
        for i in range(len(results[0].boxes)):
            box = results[0].boxes[i]
            if box.conf[0] > conf_thres:
                xmin, ymin, xmax, ymax = map(int, box.xyxy[0].cpu().numpy())
                if (xmax - xmin) * (ymax - ymin) > 2000:
                    keypoints = results[0].keypoints[i].xy.cpu().numpy()[0]
                    keypoints_conf = results[0].keypoints[i].conf.cpu().numpy()[0]
                    detections.append({
                        'box': (xmin, ymin, xmax, ymax),
                        'keypoints': keypoints,
                        'keypoints_conf': keypoints_conf
                    })
    return detections

def calculate_body_center_from_keypoints(keypoints, keypoints_conf, bbox):
    """
    ä½¿ç”¨å››ä¸ªå…³é”®ç‚¹è®¡ç®—äººä½“ä¸­å¿ƒï¼šå·¦å³è‚©è†€(5,6)å’Œå·¦å³é«‹éƒ¨(11,12)
    å¦‚æœå…³é”®ç‚¹ä¸å¯ç”¨ï¼Œåˆ™å›é€€åˆ°è¾¹ç•Œæ¡†ä¸­å¿ƒ
    """
    # COCOæ ¼å¼å…³é”®ç‚¹ç´¢å¼•
    left_shoulder, right_shoulder = 5, 6
    left_hip, right_hip = 11, 12
    
    # æ”¶é›†æœ‰æ•ˆçš„å…³é”®ç‚¹
    valid_points = []
    conf_threshold = 0.01
    
    if keypoints_conf[left_shoulder] > conf_threshold:
        valid_points.append(keypoints[left_shoulder])
    if keypoints_conf[right_shoulder] > conf_threshold:
        valid_points.append(keypoints[right_shoulder])
    if keypoints_conf[left_hip] > conf_threshold:
        valid_points.append(keypoints[left_hip])
    if keypoints_conf[right_hip] > conf_threshold:
        valid_points.append(keypoints[right_hip])
    
    # å¦‚æœæœ‰è¶³å¤Ÿçš„å…³é”®ç‚¹ï¼Œè®¡ç®—ä¸­å¿ƒ
    if len(valid_points) >= 2:
        valid_points = np.array(valid_points)
        center_x = np.mean(valid_points[:, 0])
        center_y = np.mean(valid_points[:, 1])
        return (center_x, center_y)
    else:
        # å›é€€åˆ°è¾¹ç•Œæ¡†ä¸­å¿ƒ
        xmin, ymin, xmax, ymax = bbox
        return ((xmin + xmax) / 2, (ymin + ymax) / 2)

def detect_all_persons(frame, model, conf_thres=0.5):
    """å…¼å®¹å‡½æ•°ï¼šä»poseæ£€æµ‹ä¸­æå–è¾¹ç•Œæ¡†"""
    pose_detections = detect_all_poses(frame, model, conf_thres)
    boxes = []
    for det in pose_detections:
        boxes.append(det['box'])
    return boxes

def find_center_person(frame, yolo_model):
    """åœ¨æ‰€æœ‰æ£€æµ‹åˆ°çš„äººä¸­ï¼Œæ‰¾åˆ°æœ€æ¥è¿‘ç”»é¢ä¸­å¿ƒçš„ä¸€ä¸ªï¼Œè¿”å›å§¿æ€æ£€æµ‹ç»“æœ"""
    detections = detect_all_poses(frame, yolo_model)
    if not detections: return None
    
    frame_center_x, frame_center_y = frame.shape[1] / 2, frame.shape[0] / 2
    min_dist = float('inf')
    center_detection = None
    
    for det in detections:
        # ä½¿ç”¨å…³é”®ç‚¹è®¡ç®—äººä½“ä¸­å¿ƒ
        body_center = calculate_body_center_from_keypoints(
            det['keypoints'], det['keypoints_conf'], det['box']
        )
        
        # è®¡ç®—åˆ°ç”»é¢ä¸­å¿ƒçš„è·ç¦»
        dist = math.sqrt((body_center[0] - frame_center_x)**2 + 
                        (body_center[1] - frame_center_y)**2)
        if dist < min_dist:
            min_dist = dist
            center_detection = det
    
    return center_detection


# ==============================================================================
# å¤šçº¿ç¨‹æ¡†æ¶
# ==============================================================================
class ProcessingThread(threading.Thread):
    def __init__(self, frame_queue, result_queue, stop_event, start_event, grpc_client, args, yolo_model, reid_model):
        super().__init__()
        self.frame_queue = frame_queue
        self.result_queue = result_queue
        self.stop_event = stop_event
        self.start_event = start_event
        self.grpc_client = grpc_client
        self.args = args
        self.device = torch.device(args.device)
        self.yolo_model = yolo_model.to(self.device)
        self.reid_model = reid_model.to(self.device)
        
        # åæ ‡å¯¼å‡ºå™¨ (ç”¨äºROS2é›†æˆ)
        self.coord_exporter = CoordinateExporter() if not args.no_ros_export else None
        
        # ROS2 æ§åˆ¶æ–‡ä»¶è¯»å– (ä¸å½±å“gRPCé€»è¾‘)
        self.ros_control_file = '/tmp/vision_control.json'
        self.last_ros_check_time = 0
        self.ros_command_active = False
        self.last_ros_command_active = False  # è®°å½•ä¸Šä¸€æ¬¡çš„ROS2å‘½ä»¤çŠ¶æ€
        
        # çŠ¶æ€æœºç›¸å…³
        self.state = 'IDLE'
        self.query_feats = None
        self.captured_features = []
        self.capture_start_time = 0
        self.last_capture_time = 0
        self.last_grpc_check_time = 0
        self.current_depth_frame = None
        
        # æ‰©å±•å¡å°”æ›¼æ»¤æ³¢å™¨åˆå§‹åŒ– - ä½¿ç”¨å¢å¼ºç‰ˆEKF
        print(f"ğŸ¯ ä½¿ç”¨å¢å¼ºç‰ˆå¡å°”æ›¼æ»¤æ³¢å™¨ (åŒ…å«è§’é€Ÿåº¦çš„åŒ€åŠ é€Ÿè¿åŠ¨æ¨¡å‹)")
        self.ekf = EnhancedEKF3D(
            process_noise_std=getattr(args, 'ekf_process_noise', 2.0),
            measurement_noise_std=getattr(args, 'ekf_measurement_noise', 8.0),
            initial_velocity_std=getattr(args, 'ekf_velocity_std', 0.5),
            initial_acceleration_std=getattr(args, 'ekf_acceleration_std', 0.3),
            initial_angular_velocity_std=getattr(args, 'ekf_angular_velocity_std', 0.2)
        )
        
        # å¯è§†åŒ–ç›¸å…³
        self.last_tracked_bbox = None
        self.last_tracked_kpts = None
        self.last_tracked_kpts_conf = None
        self.last_match_dist = 0.0
        self.last_coords = None
        self.last_filtered_coords = None  # æ»¤æ³¢åçš„åæ ‡
        self.last_predicted_coords = None  # é¢„æµ‹çš„åæ ‡
        self.status_message = "çŠ¶æ€: å¾…æœº (ç­‰å¾…æŒ‡ä»¤...)"
        self.fps = 0
        self.frame_count = 0
        self.start_time = time.time()
        self.enable_visualization = not args.no_viz

    def run(self):
        if self.grpc_client: self.grpc_client.connect()
        build_transforms(reidCfg)

        while not self.stop_event.is_set():
            try:
                frame, depth_frame = self.frame_queue.get(timeout=1)
                self.current_depth_frame = depth_frame
            except queue.Empty:
                continue
            
            # 1. æ‰§è¡Œæ ¸å¿ƒçŠ¶æ€é€»è¾‘
            self.handle_state(frame)

            # 2. å¦‚æœå¯ç”¨ï¼Œåˆ›å»ºå¹¶å‘é€å¯è§†åŒ–å¸§
            if self.enable_visualization:
                vis_frame = self.create_visualization(frame)
                if self.result_queue.full():
                    self.result_queue.get_nowait()
                self.result_queue.put(vis_frame)

        if self.grpc_client: self.grpc_client.disconnect()
        print("å¤„ç†çº¿ç¨‹å·²åœæ­¢ã€‚")
        
    def handle_state(self, frame):
        start_signal = self.check_start_signal()

        if self.state == 'IDLE':
            self.status_message = "çŠ¶æ€: å¾…æœº (æŒ‰Ræˆ–ç­‰å¾…gRPC/ROS2æŒ‡ä»¤)"
            if start_signal:
                self.transition_to_capturing(frame)
        
        elif self.state == 'CAPTURING':
            self.process_capturing(frame)
        elif self.state == 'TRACKING':
            self.process_tracking(frame)
            # æ£€æŸ¥gRPCåœæ­¢ä¿¡å· (ä¿æŒåŸæœ‰é€»è¾‘ä¸å˜)
            if self.grpc_client and (time.time() - self.last_grpc_check_time > 0.2):
                self.last_grpc_check_time = time.time()
                is_active, _ = self.grpc_client.get_command_state()
                if not is_active and self.grpc_client.connected and not self.last_ros_command_active:
                    print("æ”¶åˆ°gRPCåœæ­¢æŒ‡ä»¤ï¼Œè¿”å›å¾…æœºçŠ¶æ€ã€‚")
                    self.transition_to_idle()
                    return
            
            # æ£€æŸ¥ROS2åœæ­¢ä¿¡å· (æ–°å¢ï¼Œä¸å½±å“gRPC)
            # å¦‚æœä¹‹å‰æ˜¯é€šè¿‡ROS2å¯åŠ¨çš„ï¼Œæ£€æŸ¥ROS2ä¿¡å·æ˜¯å¦ä»1å˜ä¸º0
            if hasattr(self, '_started_by_ros') and self._started_by_ros:
                prev_ros_active = self.last_ros_command_active
                current_ros_active = self.check_ros_control_signal()
                # å¦‚æœROS2ä¿¡å·ä»æ¿€æ´»å˜ä¸ºéæ¿€æ´»ï¼Œåˆ™åœæ­¢è·Ÿè¸ª
                if prev_ros_active and not current_ros_active:
                    print("æ£€æµ‹åˆ°ROS2ä¿¡å·ä»1å˜ä¸º0ï¼Œè¿”å›å¾…æœºçŠ¶æ€ã€‚")
                    self.transition_to_idle()
            else:
                # å³ä½¿ä¸æ˜¯ROS2å¯åŠ¨çš„ï¼Œä¹Ÿè¦æ›´æ–°ROS2çŠ¶æ€
                self.check_ros_control_signal()

    def check_ros_control_signal(self):
        """æ£€æŸ¥ROS2æ§åˆ¶ä¿¡å· (ä¸å½±å“gRPCé€»è¾‘)"""
        if time.time() - self.last_ros_check_time < 0.5:  # æ¯0.5ç§’æ£€æŸ¥ä¸€æ¬¡
            return self.ros_command_active
            
        self.last_ros_check_time = time.time()
        
        try:
            if not os.path.exists(self.ros_control_file):
                self.ros_command_active = False
                self.last_ros_command_active = False
                return False
                
            with open(self.ros_control_file, 'r') as f:
                data = json.load(f)
                command = data.get('command', 0)
                
                new_active = (command == 1)
                
                # çŠ¶æ€æ”¹å˜æ—¶æ‰“å°æ—¥å¿—
                if new_active != self.last_ros_command_active:
                    if new_active:
                        print("æ”¶åˆ°ROS2å¼€å¯è·ŸéšæŒ‡ä»¤...")
                    else:
                        print("æ”¶åˆ°ROS2å…³é—­è·ŸéšæŒ‡ä»¤...")
                
                self.last_ros_command_active = new_active
                self.ros_command_active = new_active
                return self.ros_command_active
                
        except (json.JSONDecodeError, FileNotFoundError, KeyError):
            self.ros_command_active = False
            self.last_ros_command_active = False
            return False

    def check_start_signal(self):
        # 1. æ£€æŸ¥é”®ç›˜ä¿¡å·
        if self.start_event.is_set():
            self.start_event.clear()
            self._started_by_ros = False
            print("æ”¶åˆ° 'R' é”®ä¿¡å·ï¼Œå‡†å¤‡å¼€å§‹æ•è·...")
            return True
            
        # 2. æ£€æŸ¥gRPCä¿¡å· (ä¿æŒåŸæœ‰é€»è¾‘ä¸å˜)
        if self.grpc_client and (time.time() - self.last_grpc_check_time > 0.2):
            self.last_grpc_check_time = time.time()
            is_active, _ = self.grpc_client.get_command_state()
            if is_active:
                self._started_by_ros = False
                print("æ”¶åˆ°gRPCå¼€å§‹æŒ‡ä»¤ï¼Œå‡†å¤‡å¼€å§‹æ•è·...")
                return True
                
        # 3. æ£€æŸ¥ROS2æ§åˆ¶ä¿¡å· (æ–°å¢ï¼Œä¸å½±å“gRPC)
        prev_ros_active = self.last_ros_command_active
        current_ros_active = self.check_ros_control_signal()
        
        # åªæœ‰åœ¨ä»éæ¿€æ´»çŠ¶æ€å˜ä¸ºæ¿€æ´»çŠ¶æ€æ—¶æ‰è§¦å‘å¼€å§‹ä¿¡å·
        if current_ros_active and not prev_ros_active and self.state == 'IDLE':
            self._started_by_ros = True
            print("æ£€æµ‹åˆ°ROS2ä¿¡å·ä»0å˜ä¸º1ï¼Œå‡†å¤‡å¼€å§‹æ•è·...")
            return True
            
        return False

    def transition_to_capturing(self, frame):
        initial_detection = find_center_person(frame, self.yolo_model)
        if initial_detection is None:
            print("å¯åŠ¨å¤±è´¥ï¼šç”»é¢ä¸­å¤®æœªæ£€æµ‹åˆ°ç›®æ ‡ã€‚")
            return
        self.state = 'CAPTURING'
        self.captured_features = []
        self.capture_start_time = time.time()
        self.last_capture_time = time.time() - 1.9
        self.status_message = "collecting... (0/5)"
        print(f"ç›®æ ‡é”å®šï¼š{initial_detection['box']}ã€‚å¼€å§‹ç‰¹å¾æ•è·...")

    def process_capturing(self, frame):
        time_elapsed = time.time() - self.capture_start_time
        if time_elapsed > 3.0:
            if len(self.captured_features) > 0:
                print(f"ç‰¹å¾æ•è·å®Œæˆï¼Œå…± {len(self.captured_features)} ä¸ªã€‚æ­£åœ¨èåˆç‰¹å¾...")
                feats_tensor = torch.cat(self.captured_features, dim=0)
                avg_feat = torch.mean(feats_tensor, dim=0, keepdim=True)
                self.query_feats = F.normalize(avg_feat, dim=1, p=2)
                self.transition_to_tracking()
            else:
                print("æ•è·å¤±è´¥ï¼Œæœªé‡‡é›†åˆ°ä»»ä½•æœ‰æ•ˆç‰¹å¾ã€‚")
                self.transition_to_idle()
            return
        if len(self.captured_features) < 5 and (time.time() - self.last_capture_time) > 0.6:
            detection = find_center_person(frame, self.yolo_model)
            if detection:
                (xmin, ymin, xmax, ymax) = detection['box']
                crop_img = frame[ymin:ymax, xmin:xmax]
                if crop_img.size > 0:
                    crop_img_pil = Image.fromarray(cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB))
                    img_tensor = build_transforms(reidCfg)(crop_img_pil).unsqueeze(0).to(self.device)
                    with torch.no_grad():
                        feat = self.reid_model(img_tensor)
                    self.captured_features.append(feat)
                    self.last_capture_time = time.time()
                    self.status_message = f"collecting... ({len(self.captured_features)}/5)"
                    print(f"å·²æ•è·ç‰¹å¾ {len(self.captured_features)}/5")

    def process_tracking(self, frame):
        if self.query_feats is None:
            self.transition_to_idle()
            return
        self.status_message = "tracking..."
        person_detections = detect_all_poses(frame, self.yolo_model, self.args.conf_thres)
        best_match_info = None
        current_time = time.time()
        
        if person_detections:
            gallery_locs, gallery_feats = self.extract_gallery_features(frame, person_detections)
            if gallery_feats is not None:
                distmat = self.calculate_distance_matrix(gallery_feats)
                best_g_idx = np.argmin(distmat[0])
                min_dist = distmat[0, best_g_idx]
                if min_dist < self.args.dist_thres:
                    best_detection = person_detections[best_g_idx]
                    best_match_info = {'detection': best_detection, 'dist': min_dist}
        
        # æ›´æ–°çŠ¶æ€ç”¨äºå¯è§†åŒ–å’Œå‘é€
        if best_match_info:
            best_detection = best_match_info['detection']
            self.last_tracked_bbox = best_detection['box']
            self.last_tracked_kpts = best_detection['keypoints']
            self.last_tracked_kpts_conf = best_detection['keypoints_conf']
            self.last_match_dist = best_match_info['dist']
            
            # ä½¿ç”¨å…³é”®ç‚¹è®¡ç®—äººä½“ä¸­å¿ƒ
            body_center = calculate_body_center_from_keypoints(
                best_detection['keypoints'], 
                best_detection['keypoints_conf'], 
                best_detection['box']
            )
            
            size = (self.last_tracked_bbox[2] - self.last_tracked_bbox[0], 
                   self.last_tracked_bbox[3] - self.last_tracked_bbox[1])
            
            if self.current_depth_frame is not None:
                coords = calculate_3d_coordinates(self.current_depth_frame, body_center, size)
                if coords != (0,0,0):
                    self.last_coords = coords
                    
                    # ä½¿ç”¨å¡å°”æ›¼æ»¤æ³¢å™¨å¤„ç†åæ ‡
                    measurement = np.array([coords[0], coords[1], coords[2]])
                    
                    if not self.ekf.is_initialized():
                        # åˆå§‹åŒ–å¡å°”æ›¼æ»¤æ³¢å™¨
                        self.ekf.initialize(measurement, current_time)
                        self.last_filtered_coords = coords
                        self.last_predicted_coords = coords
                        print(f"ğŸ¯ å¡å°”æ›¼æ»¤æ³¢å™¨å·²åˆå§‹åŒ–: [{coords[0]:.2f}, {coords[1]:.2f}, {coords[2]:.2f}]")
                    else:
                        # é¢„æµ‹å’Œæ›´æ–°
                        self.ekf.predict(current_time)
                        filtered_state = self.ekf.update(measurement)
                        self.last_filtered_coords = self.ekf.get_current_position()
                        self.last_predicted_coords = self.ekf.predict_future_position(0.5)  # é¢„æµ‹0.5ç§’åçš„ä½ç½®

                        # æ‰“å°è°ƒè¯•ä¿¡æ¯
                        velocity = self.ekf.get_current_velocity()
                        print(f"ğŸ“ åŸå§‹: [{coords[0]:.2f}, {coords[1]:.2f}, {coords[2]:.2f}] | "
                              f"æ»¤æ³¢: [{self.last_filtered_coords[0]:.2f}, {self.last_filtered_coords[1]:.2f}, {self.last_filtered_coords[2]:.2f}] | "
                              f"é€Ÿåº¦: [{velocity[0]:.2f}, {velocity[1]:.2f}, {velocity[2]:.2f}]")
                else:
                    self.last_coords = None
                    # å¤„ç†ç›®æ ‡ä¸¢å¤±æƒ…å†µ
                    if self.ekf.is_initialized():
                        predicted_pos = self.ekf.handle_lost_target(current_time)
                        if predicted_pos is not None:
                            self.last_filtered_coords = predicted_pos
                            self.last_predicted_coords = self.ekf.predict_future_position(0.5)
                            print(f"ğŸ” ç›®æ ‡ä¸¢å¤±ï¼Œä½¿ç”¨é¢„æµ‹ä½ç½®: [{predicted_pos[0]:.2f}, {predicted_pos[1]:.2f}, {predicted_pos[2]:.2f}]")
                        else:
                            self.last_filtered_coords = None
                            self.last_predicted_coords = None
            else:
                self.last_coords = None
                self.last_filtered_coords = None
                self.last_predicted_coords = None
        else:
            self.last_tracked_bbox = None
            self.last_tracked_kpts = None
            self.last_tracked_kpts_conf = None
            self.last_coords = None
            
            # å¤„ç†ç›®æ ‡ä¸¢å¤±æƒ…å†µ
            if self.ekf.is_initialized():
                predicted_pos = self.ekf.handle_lost_target(current_time)
                if predicted_pos is not None:
                    self.last_filtered_coords = predicted_pos
                    self.last_predicted_coords = self.ekf.predict_future_position(0.5)
                    print(f"ğŸ” ç›®æ ‡ä¸¢å¤±ï¼Œä½¿ç”¨é¢„æµ‹ä½ç½®: [{predicted_pos[0]:.2f}, {predicted_pos[1]:.2f}, {predicted_pos[2]:.2f}]")
                else:
                    self.last_filtered_coords = None
                    self.last_predicted_coords = None
                    self.ekf.reset()  # é‡ç½®æ»¤æ³¢å™¨
                    print("ğŸ”„ ç›®æ ‡ä¸¢å¤±æ—¶é—´è¿‡é•¿ï¼Œæ»¤æ³¢å™¨å·²é‡ç½®")
            else:
                self.last_filtered_coords = None
                self.last_predicted_coords = None

        # å‘é€åæ ‡ - ä¼˜å…ˆå‘é€æ»¤æ³¢åçš„åæ ‡ï¼Œå…¶æ¬¡æ˜¯åŸå§‹åæ ‡ï¼Œæœ€åæ˜¯ (0, 0, 0)
        coords_to_send = (0.0, 0.0, 0.0)
        if self.last_filtered_coords:
            coords_to_send = self.last_filtered_coords
        elif self.last_coords:
            coords_to_send = self.last_coords
        
        if self.grpc_client:
            self.grpc_client.send_target_coordinates(coords_to_send)
            
        # å¯¼å‡ºåæ ‡åˆ°æ–‡ä»¶ (ç”¨äºROS2é›†æˆ) - ä½¿ç”¨æ»¤æ³¢åçš„åæ ‡
        if self.coord_exporter:
            self.coord_exporter.export_coordinates(coords_to_send)

    def transition_to_tracking(self):
        self.state = 'TRACKING'
    def transition_to_idle(self):
        self.state = 'IDLE'
        self.query_feats = None
        self.captured_features = []
        # é‡ç½®å¡å°”æ›¼æ»¤æ³¢å™¨
        self.ekf.reset()
        self.last_filtered_coords = None
        self.last_predicted_coords = None
        print("ğŸ”„ è½¬æ¢åˆ°å¾…æœºçŠ¶æ€ï¼Œå¡å°”æ›¼æ»¤æ³¢å™¨å·²é‡ç½®")

    def create_visualization(self, frame):
        vis_frame = frame.copy()
        
        # æ ¹æ®çŠ¶æ€ç»˜åˆ¶ä¸åŒçš„æ¡†å’Œå§¿æ€
        if self.state == 'CAPTURING':
            detection = find_center_person(vis_frame, self.yolo_model)
            if detection: 
                draw_pose_on_person(vis_frame, detection, "Capturing...", (0, 165, 255))
        elif self.state == 'TRACKING':
            # ç»˜åˆ¶æ‰€æœ‰æ£€æµ‹åˆ°çš„äººç‰©ï¼ˆæµ…è‰²ï¼‰
            all_detections = detect_all_poses(vis_frame, self.yolo_model, self.args.conf_thres)
            for detection in all_detections:
                # ç”¨æµ…è‰²ç»˜åˆ¶æ‰€æœ‰æ£€æµ‹åˆ°çš„äºº
                draw_skeleton(vis_frame, detection['keypoints'], detection['keypoints_conf'])
                plot_one_box(detection['box'], vis_frame, label="Person", color=(128, 128, 128))
            
            # ç»˜åˆ¶è·Ÿè¸ªç›®æ ‡ï¼ˆé«˜äº®ï¼‰
            if self.last_tracked_bbox and self.last_tracked_kpts is not None:
                label = f"Target | Dist: {self.last_match_dist:.2f}"
                
                # æ˜¾ç¤ºåŸå§‹åæ ‡
                if self.last_coords:
                    label += f' | Raw: {self.last_coords[0]:.1f}, {self.last_coords[1]:.1f}, {self.last_coords[2]:.1f}m'
                
                # æ˜¾ç¤ºæ»¤æ³¢åçš„åæ ‡
                if self.last_filtered_coords:
                    label += f' | Filtered: {self.last_filtered_coords[0]:.1f}, {self.last_filtered_coords[1]:.1f}, {self.last_filtered_coords[2]:.1f}m'
                
                # ç»˜åˆ¶è·Ÿè¸ªç›®æ ‡çš„è¾¹ç•Œæ¡†
                plot_one_box(self.last_tracked_bbox, vis_frame, label=label, color=(0,255,0))
                
                # åˆ›å»ºä¼ªæ£€æµ‹å¯¹è±¡æ¥ç»˜åˆ¶éª¨æ¶
                tracked_detection = {
                    'box': self.last_tracked_bbox,
                    'keypoints': self.last_tracked_kpts,
                    'keypoints_conf': self.last_tracked_kpts_conf
                }
                
                # ç»˜åˆ¶è·Ÿè¸ªç›®æ ‡çš„éª¨æ¶å’Œå…³é”®ç‚¹ï¼ˆé«˜äº®æ˜¾ç¤ºï¼‰
                draw_skeleton(vis_frame, tracked_detection['keypoints'], tracked_detection['keypoints_conf'])
                draw_keypoints(vis_frame, tracked_detection['keypoints'], tracked_detection['keypoints_conf'])
                
                # ç»˜åˆ¶äººä½“ä¸­å¿ƒç‚¹ï¼ˆç‰¹æ®Šæ ‡è®°ï¼‰
                body_center = calculate_body_center_from_keypoints(
                    tracked_detection['keypoints'], tracked_detection['keypoints_conf'], tracked_detection['box']
                )
                center_x, center_y = int(body_center[0]), int(body_center[1])
                cv2.circle(vis_frame, (center_x, center_y), 8, (0, 255, 255), -1)  # é»„è‰²åœ†ç‚¹
                cv2.circle(vis_frame, (center_x, center_y), 10, (0, 0, 0), 2)     # é»‘è‰²è¾¹æ¡†
                
                # ç»˜åˆ¶é¢„æµ‹ä½ç½®çš„æŠ•å½±ï¼ˆå¦‚æœæœ‰ï¼‰
                if self.last_predicted_coords and self.current_depth_frame is not None:
                    # å°†3Dé¢„æµ‹ä½ç½®æŠ•å½±å›å›¾åƒåæ ‡
                    fx, fy = 734.357, 734.629
                    cx, cy = 816.469, 642.979
                    
                    X_world, Y_world, Z_world = self.last_predicted_coords
                    Z_cam = X_world
                    X_cam = -Y_world
                    Y_cam = -Z_world
                    
                    if Z_cam > 0.3:  # åªæœ‰åœ¨åˆç†è·ç¦»å†…æ‰ç»˜åˆ¶
                        u_pred = int(X_cam * fx / Z_cam + cx)
                        v_pred = int(Y_cam * fy / Z_cam + cy)
                        
                        # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
                        if 0 <= u_pred < vis_frame.shape[1] and 0 <= v_pred < vis_frame.shape[0]:
                            cv2.circle(vis_frame, (u_pred, v_pred), 12, (255, 255, 0), 3)  # é’è‰²é¢„æµ‹åœ†åœˆ
                            cv2.putText(vis_frame, "Pred", (u_pred-20, v_pred-15), 
                                      cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
        
        # ç»˜åˆ¶å›ºå®šçš„UIå…ƒç´ 
        self.frame_count += 1
        if time.time() - self.start_time > 1:
            self.fps = self.frame_count / (time.time() - self.start_time)
            self.start_time = time.time()
            self.frame_count = 0
        
        cv2.putText(vis_frame, f"FPS: {self.fps:.1f}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        cv2.putText(vis_frame, self.status_message, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        
        # æ˜¾ç¤ºå¡å°”æ›¼æ»¤æ³¢å™¨çŠ¶æ€
        if self.ekf.is_initialized():
            uncertainty = self.ekf.get_position_uncertainty()
            velocity = self.ekf.get_current_velocity()
            ekf_status = f"Enhanced EKF: Init | Unc: {uncertainty:.3f} | Vel: [{velocity[0]:.2f}, {velocity[1]:.2f}, {velocity[2]:.2f}]"
            cv2.putText(vis_frame, ekf_status, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
        else:
            cv2.putText(vis_frame, "Enhanced EKF: Not Initialized", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (128, 128, 128), 1)
        
        # æ˜¾ç¤ºå…³é”®ç‚¹ä¿¡æ¯
        if self.state == 'TRACKING' and self.last_tracked_kpts is not None:
            valid_kpts = sum(1 for conf in self.last_tracked_kpts_conf if conf > 0.5)
            kpt_info = f"Keypoints: {valid_kpts}/17"
            cv2.putText(vis_frame, kpt_info, (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
        return vis_frame

    def extract_gallery_features(self, frame, person_detections):
        """ä»å§¿æ€æ£€æµ‹ç»“æœä¸­æå–ç‰¹å¾"""
        gallery_locs, gallery_img_tensors = [], []
        for detection in person_detections:
            xmin, ymin, xmax, ymax = detection['box']
            crop_img = frame[ymin:ymax, xmin:xmax]
            if crop_img.size > 0:
                gallery_locs.append((xmin, ymin, xmax, ymax))
                crop_img_pil = Image.fromarray(cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB))
                gallery_img_tensors.append(build_transforms(reidCfg)(crop_img_pil).unsqueeze(0))
        if not gallery_img_tensors: return None, None
        gallery_img = torch.cat(gallery_img_tensors, dim=0).to(self.device)
        with torch.no_grad():
            gallery_feats = self.reid_model(gallery_img)
            gallery_feats = F.normalize(gallery_feats, dim=1, p=2)
        return gallery_locs, gallery_feats

    def calculate_distance_matrix(self, gallery_feats):
        m, n = self.query_feats.shape[0], gallery_feats.shape[0]
        distmat = torch.pow(self.query_feats, 2).sum(dim=1, keepdim=True).expand(m, n) + \
                  torch.pow(gallery_feats, 2).sum(dim=1, keepdim=True).expand(n, m).t()
        distmat.addmm_(self.query_feats, gallery_feats.t(), beta=1, alpha=-2)
        return distmat.cpu().numpy()

# ==============================================================================
# ROS2 èŠ‚ç‚¹å’Œä¸»ç¨‹åº
# ==============================================================================
class ImageSubscriberNode(Node):
    def __init__(self, args):
        super().__init__('reid_tracker_node')
        self.get_logger().info("=== ReID ROS2 è·Ÿè¸ªèŠ‚ç‚¹å¯åŠ¨ ===")
        
        self.bridge = CvBridge()
        self.frame_queue = queue.Queue(maxsize=5)
        self.result_queue = queue.Queue(maxsize=5) if not args.no_viz else None
        self.stop_event = threading.Event()
        self.start_event = threading.Event()
        
        # åŠ è½½æ¨¡å‹
        self.get_logger().info("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        try:
            self.yolo_model = YOLO(args.model_path)
            self.reid_model = build_model(reidCfg, num_classes=1501)
            self.reid_model.load_param(reidCfg.TEST.WEIGHT)
            self.reid_model.eval()
            self.get_logger().info("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            self.get_logger().error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e

        # åˆå§‹åŒ–gRPCå®¢æˆ·ç«¯
        self.grpc_client = TrackingGRPCClient(args.grpc_server) if not args.no_grpc else None
        
        # å¯åŠ¨å¤„ç†çº¿ç¨‹
        self.processing_thread = ProcessingThread(
            self.frame_queue, self.result_queue, self.stop_event, 
            self.start_event, self.grpc_client, args, 
            self.yolo_model, self.reid_model
        )
        self.processing_thread.start()
        self.get_logger().info("âœ“ åå°å¤„ç†çº¿ç¨‹å·²å¯åŠ¨...")

        # è®¾ç½®è®¢é˜…å™¨
        self.color_sub = message_filters.Subscriber(self, RosImage, '/odin1/image_undistorted')
        self.depth_sub = message_filters.Subscriber(self, RosImage, '/odin1/depth/image_raw')

        # æ—¶é—´åŒæ­¥å™¨
        self.time_synchronizer = message_filters.ApproximateTimeSynchronizer(
            [self.color_sub, self.depth_sub],
            queue_size=10,
            slop=0.1  # å…è®¸0.1ç§’çš„æ—¶é—´æˆ³å·®å¼‚
        )
        self.time_synchronizer.registerCallback(self.image_callback)
        self.get_logger().info("âœ“ å·²è®¢é˜…å½©è‰²å’Œæ·±åº¦å›¾åƒè¯é¢˜ï¼Œç­‰å¾…åŒæ­¥æ¶ˆæ¯...")

    def image_callback(self, color_msg, depth_msg):
        try:
            # å°†ROSå›¾åƒæ¶ˆæ¯è½¬æ¢ä¸ºOpenCVæ ¼å¼
            color_frame = self.bridge.imgmsg_to_cv2(color_msg, "bgr8")
            # æ·±åº¦å›¾æ˜¯32FC1æ ¼å¼ï¼Œç›´æ¥è½¬æ¢
            depth_frame = self.bridge.imgmsg_to_cv2(depth_msg, "32FC1")
        except CvBridgeError as e:
            self.get_logger().error(f"CvBridgeè½¬æ¢é”™è¯¯: {e}")
            return
        
        # å°†å¸§æ”¾å…¥é˜Ÿåˆ—ä¾›å¤„ç†çº¿ç¨‹ä½¿ç”¨
        if self.frame_queue.full():
            self.frame_queue.get_nowait() # å¦‚æœé˜Ÿåˆ—æ»¡äº†ï¼Œä¸¢å¼ƒæ—§çš„å¸§
        self.frame_queue.put((color_frame, depth_frame))

    def stop_all_threads(self):
        self.get_logger().info("æ­£åœ¨åœæ­¢æ‰€æœ‰çº¿ç¨‹...")
        self.stop_event.set()
        self.processing_thread.join(timeout=5)
        if self.processing_thread.is_alive():
            self.get_logger().warn("å¤„ç†çº¿ç¨‹æœªèƒ½æ­£å¸¸åœæ­¢ã€‚")

def main(args):
    rclpy.init(args=None)
    
    try:
        reid_node = ImageSubscriberNode(args)
    except Exception as e:
        print(f"èŠ‚ç‚¹åˆå§‹åŒ–å¤±è´¥: {e}")
        rclpy.shutdown()
        return

    if not args.no_viz:
        window_name = 'ReID ROS2 Tracking'
        cv2.namedWindow(window_name, cv2.WINDOW_AUTOSIZE)
        
        while rclpy.ok():
            rclpy.spin_once(reid_node, timeout_sec=0.01)
            try:
                display_frame = reid_node.result_queue.get_nowait()
                cv2.imshow(window_name, display_frame)
            except queue.Empty:
                pass

            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord('r'):
                print("é”®ç›˜ 'R' å·²æŒ‰ä¸‹ï¼Œå‘é€å¼€å§‹ä¿¡å·...")
                reid_node.start_event.set()
        
        cv2.destroyAllWindows()
    else:
        # åœ¨æ— å¤´æ¨¡å¼ä¸‹è¿è¡Œ
        try:
            rclpy.spin(reid_node)
        except KeyboardInterrupt:
            pass

    # æ¸…ç†
    reid_node.stop_all_threads()
    reid_node.destroy_node()
    rclpy.shutdown()
    print("ç¨‹åºå·²å®‰å…¨é€€å‡ºã€‚")

def parse_args():
    parser = argparse.ArgumentParser(description='ROS2 ReID Auto Tracking with Pose Detection and Kalman Filter')
    parser.add_argument('--model-path', type=str, default='models/yolo11n-pose.pt', help='YOLOv11-Poseæ¨¡å‹è·¯å¾„')
    parser.add_argument('--dist-thres', type=float, default=1.1, help='ReIDè·ç¦»é˜ˆå€¼')
    parser.add_argument('--conf-thres', type=float, default=0.5, help='YOLOæ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼')
    parser.add_argument('--device', type=str, default=None, help='è®¡ç®—è®¾å¤‡ (e.g., cpu, cuda:0)')
    parser.add_argument('--grpc-server', default='localhost:50051', help='gRPCæœåŠ¡å™¨åœ°å€')
    parser.add_argument('--no-viz', action='store_true', help='ç¦ç”¨å¯è§†åŒ–ç•Œé¢')
    parser.add_argument('--no-grpc', action='store_true', help='ç¦ç”¨gRPCé€šä¿¡')
    parser.add_argument('--no-ros-export', action='store_true', help='ç¦ç”¨ROS2åæ ‡å¯¼å‡º')
    
    # å¡å°”æ›¼æ»¤æ³¢å™¨å‚æ•°
    parser.add_argument('--ekf-process-noise', type=float, default=0.5, help='å¡å°”æ›¼æ»¤æ³¢å™¨è¿‡ç¨‹å™ªå£°æ ‡å‡†å·®')
    parser.add_argument('--ekf-measurement-noise', type=float, default=3.0, help='å¡å°”æ›¼æ»¤æ³¢å™¨æµ‹é‡å™ªå£°æ ‡å‡†å·®')
    parser.add_argument('--ekf-velocity-std', type=float, default=0.5, help='å¡å°”æ›¼æ»¤æ³¢å™¨åˆå§‹é€Ÿåº¦ä¸ç¡®å®šæ€§æ ‡å‡†å·®')
    parser.add_argument('--ekf-acceleration-std', type=float, default=0.3, help='å¡å°”æ›¼æ»¤æ³¢å™¨åˆå§‹åŠ é€Ÿåº¦ä¸ç¡®å®šæ€§æ ‡å‡†å·®')
    parser.add_argument('--ekf-angular-velocity-std', type=float, default=0.2, help='å¡å°”æ›¼æ»¤æ³¢å™¨åˆå§‹è§’é€Ÿåº¦ä¸ç¡®å®šæ€§æ ‡å‡†å·®')

    return parser.parse_args()

if __name__ == '__main__':
    args = parse_args()
    if args.device is None:
        args.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨çš„è®¡ç®—è®¾å¤‡: {args.device}")
    
    # æ˜¾ç¤ºå¡å°”æ›¼æ»¤æ³¢å™¨é…ç½®ä¿¡æ¯
    print(f"ğŸ¯ å¡å°”æ›¼æ»¤æ³¢å™¨é…ç½®: Enhanced EKF (åŒ€åŠ é€Ÿè¿åŠ¨æ¨¡å‹)")
    print(f"   è¿‡ç¨‹å™ªå£°: {args.ekf_process_noise}")
    print(f"   æµ‹é‡å™ªå£°: {args.ekf_measurement_noise}")
    print(f"   é€Ÿåº¦ä¸ç¡®å®šæ€§: {args.ekf_velocity_std}")
    print(f"   åŠ é€Ÿåº¦ä¸ç¡®å®šæ€§: {args.ekf_acceleration_std}")
    print(f"   è§’é€Ÿåº¦ä¸ç¡®å®šæ€§: {args.ekf_angular_velocity_std}")
    
    main(args)
